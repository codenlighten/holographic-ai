{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff6fc52d",
   "metadata": {},
   "source": [
    "# HAMT A100 Training - Google Colab\n",
    "\n",
    "**Holographic Associative Memory Transformers**  \n",
    "Train competitive, efficient LLMs with O(1) memory complexity\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "This notebook trains HAMT models on Google Colab A100 GPU:\n",
    "- **50M params**: Quick validation (~30 min)\n",
    "- **100M params**: Competitive model (~6 hours)\n",
    "- **200M params**: Large-scale model (~12 hours)\n",
    "\n",
    "**Expected Results**: Match GPT-2 quality with 2-5√ó speedup and 20-50% fewer parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76675ec1",
   "metadata": {},
   "source": [
    "## üöÄ Step 1: Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340e5cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nüöÄ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU'}\")\n",
    "print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\" if torch.cuda.is_available() else \"\")\n",
    "print(f\"   CUDA: {torch.version.cuda}\" if torch.cuda.is_available() else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac16ea67",
   "metadata": {},
   "source": [
    "## üì¶ Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed41324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers datasets wandb accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f7ab7a",
   "metadata": {},
   "source": [
    "## üì• Step 3: Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f35d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone HAMT repository\n",
    "!git clone https://github.com/codenlighten/holographic-ai.git\n",
    "%cd holographic-ai\n",
    "\n",
    "# Verify files\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceae515",
   "metadata": {},
   "source": [
    "## üîß Step 4: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661c769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to Python path\n",
    "import sys\n",
    "sys.path.append('/content/holographic-ai')\n",
    "\n",
    "# Test import\n",
    "from src.hamt.model import HAMTModel\n",
    "from src.hamt.config import HAMTConfig\n",
    "print(\"‚úÖ HAMT modules loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f047f4",
   "metadata": {},
   "source": [
    "## üéØ Step 5A: Quick Validation (50M Model - 30 min)\n",
    "\n",
    "**Purpose**: Validate setup before long training run  \n",
    "**Time**: ~30 minutes  \n",
    "**Dataset**: WikiText-2 (small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10481c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test configuration\n",
    "!python train_gpu.py \\\n",
    "    --hidden_dim 384 \\\n",
    "    --hcm_dim 1536 \\\n",
    "    --num_layers 6 \\\n",
    "    --num_slots 16 \\\n",
    "    --dataset wikitext-2 \\\n",
    "    --max_seq_length 512 \\\n",
    "    --batch_size 32 \\\n",
    "    --num_epochs 3 \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --mixed_precision bf16 \\\n",
    "    --compile_model \\\n",
    "    --output_dir outputs/test_50m \\\n",
    "    --run_name hamt-test-50m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088d9408",
   "metadata": {},
   "source": [
    "## üèÜ Step 5B: Main Training (100M Model - 6 hours)\n",
    "\n",
    "**Purpose**: Competitive model for benchmarking  \n",
    "**Time**: ~6 hours  \n",
    "**Dataset**: WikiText-103 (standard benchmark)  \n",
    "**Expected**: Perplexity < 28, competitive with GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c45e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Setup Weights & Biases for monitoring\n",
    "import wandb\n",
    "wandb.login()  # Enter your API key when prompted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca398b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training configuration (100M parameters)\n",
    "!python train_gpu.py \\\n",
    "    --hidden_dim 512 \\\n",
    "    --hcm_dim 2048 \\\n",
    "    --num_layers 8 \\\n",
    "    --num_slots 16 \\\n",
    "    --dataset wikitext-103 \\\n",
    "    --max_seq_length 1024 \\\n",
    "    --batch_size 128 \\\n",
    "    --num_epochs 10 \\\n",
    "    --learning_rate 3e-4 \\\n",
    "    --mixed_precision bf16 \\\n",
    "    --compile_model \\\n",
    "    --use_wandb \\\n",
    "    --wandb_project hamt-competitive-llm \\\n",
    "    --output_dir outputs/hamt_100m \\\n",
    "    --run_name hamt-100m-wikitext103"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d37e154",
   "metadata": {},
   "source": [
    "## üöÄ Step 5C: Ambitious Training (200M Model - 12 hours)\n",
    "\n",
    "**Purpose**: Large-scale competitive model  \n",
    "**Time**: ~12 hours  \n",
    "**Dataset**: WikiText-103  \n",
    "**Expected**: Perplexity < 25, approaching GPT-2 Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bbeb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large model configuration (200M parameters)\n",
    "!python train_gpu.py \\\n",
    "    --hidden_dim 768 \\\n",
    "    --hcm_dim 3072 \\\n",
    "    --num_layers 12 \\\n",
    "    --num_slots 16 \\\n",
    "    --dataset wikitext-103 \\\n",
    "    --max_seq_length 1024 \\\n",
    "    --batch_size 64 \\\n",
    "    --num_epochs 10 \\\n",
    "    --learning_rate 2e-4 \\\n",
    "    --mixed_precision bf16 \\\n",
    "    --compile_model \\\n",
    "    --gradient_checkpointing \\\n",
    "    --use_wandb \\\n",
    "    --wandb_project hamt-competitive-llm \\\n",
    "    --output_dir outputs/hamt_200m \\\n",
    "    --run_name hamt-200m-wikitext103"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6046309e",
   "metadata": {},
   "source": [
    "## üìä Step 6: Monitor Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde55993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training progress\n",
    "!tail -20 outputs/hamt_100m/training.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336d281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor GPU utilization\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba81fd85",
   "metadata": {},
   "source": [
    "## üî¨ Step 7: Baseline Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f17968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare HAMT with baseline transformers\n",
    "!python compare_baselines.py \\\n",
    "    --hamt_checkpoint outputs/hamt_100m/checkpoints/best.pt \\\n",
    "    --baselines distilgpt2 gpt2 \\\n",
    "    --dataset wikitext-2 \\\n",
    "    --device cuda \\\n",
    "    --output_dir comparison_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc6dbba",
   "metadata": {},
   "source": [
    "## üìà Step 8: View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc83b71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display comparison results\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('comparison_results/comparison_results.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Create comparison table\n",
    "df = pd.DataFrame(results).T\n",
    "print(\"\\nüìä Model Comparison Results:\")\n",
    "print(\"=\" * 80)\n",
    "print(df[['parameters', 'perplexity', 'throughput', 'peak_memory_gb']].to_string())\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2b82e8",
   "metadata": {},
   "source": [
    "## üíæ Step 9: Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c519d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package results for download\n",
    "!zip -r results.zip \\\n",
    "    outputs/hamt_100m/checkpoints/best.pt \\\n",
    "    outputs/hamt_100m/checkpoints/best_config.json \\\n",
    "    comparison_results/ \\\n",
    "    outputs/hamt_100m/*.log\n",
    "\n",
    "print(\"‚úÖ Results packaged in results.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ee6f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results\n",
    "from google.colab import files\n",
    "files.download('results.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e23e8c",
   "metadata": {},
   "source": [
    "## üß™ Step 10: Test Generation (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc84518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test text generation with trained model\n",
    "import torch\n",
    "from src.hamt.model import HAMTModel\n",
    "from src.hamt.config import HAMTConfig\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load model\n",
    "checkpoint = torch.load('outputs/hamt_100m/checkpoints/best.pt')\n",
    "config = HAMTConfig(**checkpoint['config'])\n",
    "model = HAMTModel(config).cuda()\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Generate text\n",
    "prompt = \"Once upon a time\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt').cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_ids, max_length=100, temperature=0.8)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nüìù Generated Text:\")\n",
    "print(\"=\" * 80)\n",
    "print(generated_text)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5498d6a",
   "metadata": {},
   "source": [
    "## üêõ Troubleshooting\n",
    "\n",
    "### Out of Memory (OOM)\n",
    "```python\n",
    "# Reduce batch size\n",
    "--batch_size 64  # or 32\n",
    "\n",
    "# Reduce sequence length\n",
    "--max_seq_length 512\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "--gradient_checkpointing\n",
    "```\n",
    "\n",
    "### Slow Training\n",
    "```python\n",
    "# Ensure BF16 is enabled (faster on A100)\n",
    "--mixed_precision bf16\n",
    "\n",
    "# Enable model compilation\n",
    "--compile_model\n",
    "\n",
    "# Increase batch size (if memory allows)\n",
    "--batch_size 256\n",
    "```\n",
    "\n",
    "### Colab Disconnection\n",
    "- Training auto-saves checkpoints every epoch\n",
    "- Can resume from latest checkpoint (feature to be added)\n",
    "- Or restart from scratch (checkpoints preserved in outputs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae05e42",
   "metadata": {},
   "source": [
    "## üìä Expected Results\n",
    "\n",
    "### 100M Model (6 hours)\n",
    "- **Perplexity**: ~25-28 on WikiText-103\n",
    "- **Throughput**: 4000-6000 tokens/sec\n",
    "- **Memory**: 6-8 GB VRAM\n",
    "- **Quality**: Comparable to GPT-2 Small (124M)\n",
    "- **Speedup**: 2-3√ó faster than GPT-2\n",
    "\n",
    "### 200M Model (12 hours)\n",
    "- **Perplexity**: ~22-25 on WikiText-103\n",
    "- **Throughput**: 3000-5000 tokens/sec\n",
    "- **Memory**: 12-16 GB VRAM\n",
    "- **Quality**: Approaching GPT-2 Medium (355M)\n",
    "- **Speedup**: 2-4√ó faster than GPT-2 Medium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afde134",
   "metadata": {},
   "source": [
    "## ‚úÖ Success Criteria\n",
    "\n",
    "**Minimum Success**:\n",
    "- ‚úÖ Training completes without OOM\n",
    "- ‚úÖ Loss decreases steadily\n",
    "- ‚úÖ Perplexity < 35\n",
    "- ‚úÖ Faster than baseline\n",
    "\n",
    "**Good Success**:\n",
    "- ‚úÖ Perplexity < 30\n",
    "- ‚úÖ 2√ó faster than baseline\n",
    "- ‚úÖ Lower memory usage\n",
    "- ‚úÖ All checkpoints saved\n",
    "\n",
    "**Excellent Success**:\n",
    "- ‚úÖ Perplexity < 28\n",
    "- ‚úÖ 3-5√ó faster than baseline\n",
    "- ‚úÖ Competitive with GPT-2\n",
    "- ‚úÖ Publication-ready results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd2fc96",
   "metadata": {},
   "source": [
    "## üìö Additional Resources\n",
    "\n",
    "- **GitHub**: https://github.com/codenlighten/holographic-ai\n",
    "- **Whitepaper**: See `WHITEPAPER.md` in repo\n",
    "- **Documentation**: See `README.md` and `USAGE_GUIDE.md`\n",
    "- **Issues**: Report at GitHub Issues\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck with your training! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
